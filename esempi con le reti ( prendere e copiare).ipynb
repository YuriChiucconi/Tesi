{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nets_comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6r831NAZXNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8dd3ba68-d9e7-4fc5-eea9-43fd9e24d322"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kBz_l3kZqcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "from keras.initializers import he_normal\n",
        "\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "from keras.layers import Activation # linear, tanh, softmax\n",
        "from keras.layers import ReLU, LeakyReLU, ELU, PReLU\n",
        "\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UiXGxFuwxDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = np.load(\"xtrain.npy\")\n",
        "ytrain = np.load(\"ytrain.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcsW6tVkxaD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import timeit\n",
        "\n",
        "def train_model(xtrain, ytrain, layers,\n",
        "                activation,\n",
        "                optimizer,\n",
        "                batch_normalization=False, \n",
        "                dropout=False,\n",
        "                epochs=1000,\n",
        "                batch_size=500,\n",
        "                patience=250):\n",
        "  \n",
        "    assert len(layers)>0\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(layers[0], input_dim=xtrain.shape[1], init=he_normal(seed=None)))\n",
        "    if(batch_normalization):\n",
        "       model.add(BatchNormalization())\n",
        "    model.add(activation)\n",
        "    if(dropout):\n",
        "       model.add(Dropout(0.5))\n",
        "\n",
        "    for N in layers[1:]:\n",
        "       model.add(Dense(N, init=he_normal(seed=None)))\n",
        "       if(batch_normalization):\n",
        "          model.add(BatchNormalization())\n",
        "       model.add(activation)\n",
        "       if(dropout):\n",
        "          model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(ytrain.shape[1], init=he_normal(seed=None)))\n",
        "    model.add(Activation('linear'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    \n",
        "    history = model.fit(xtrain, ytrain, \n",
        "                        validation_split=0.2, \n",
        "                        epochs=epochs, \n",
        "                        batch_size=batch_size, \n",
        "                        verbose=0, \n",
        "                        callbacks=[early_stopping])\n",
        "    \n",
        "    stop = timeit.default_timer()\n",
        "      \n",
        "    training_time = np.round((stop-start)/60 , 2)\n",
        "\n",
        "    return model, history, training_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCfNaTnC7cP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = plt.figure(figsize=(30,30))\n",
        "_ = plt.title(\"3 hidden layers with 128 neurons each\")\n",
        "_ = plt.ylabel('Loss')\n",
        "_ = plt.xlabel('Epochs')\n",
        "\n",
        "out_file = open('training_times.txt', 'w')\n",
        "\n",
        "for ACT in [ReLU, LeakyReLU, ELU, PReLU]:\n",
        "  for OPT in [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]:\n",
        "    for DROP in [False, True]:\n",
        "      for BATCH in [False, True]:\n",
        "            name = ''\n",
        "            if(BATCH):\n",
        "              name += 'BATCH_'\n",
        "            name += ACT.__name__\n",
        "            if(DROP):\n",
        "              name += '_DROP'\n",
        "            name += '_'\n",
        "            name += OPT.__name__\n",
        "            \n",
        "            model, history, training_time = train_model(xtrain=xtrain,\n",
        "                                                        ytrain=ytrain,\n",
        "                                                        layers=[128,128,128],\n",
        "                                                        activation=ACT(),\n",
        "                                                        optimizer=OPT(),\n",
        "                                                        dropout=DROP,\n",
        "                                                        batch_normalization=BATCH,\n",
        "                                                        epochs=3000)\n",
        "            \n",
        "            model.save(name+'.h5')\n",
        "            out_file.write(name+' ---> '+str(training_time)+' minutes\\n')\n",
        "\n",
        "            _ = plt.semilogy(history.history['loss'],label=name+' train')\n",
        "            _ = plt.semilogy(history.history['val_loss'],label=name+' test')\n",
        "\n",
        "out_file.close()\n",
        "_ = plt.legend(loc='upper right')\n",
        "_ = plt.savefig('losses_graphs.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}